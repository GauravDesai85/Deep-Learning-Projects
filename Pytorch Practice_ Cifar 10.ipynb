{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Practice- CIFAR 10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import time\n",
    "import torchvision\n",
    "\n",
    "cuda_available = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Resnet Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResblock(nn.Module):\n",
    "    def __init__(self, in_ch,out_ch,stride=1):\n",
    "        \n",
    "        super(MyResblock,self).__init__()\n",
    "        # Conv layer 1\n",
    "        self.conv1=nn.Conv2d(\n",
    "            in_channels=in_ch,out_channels=out_ch,kernel_size=3,padding=1,stride=stride,bias=False\n",
    "        )\n",
    "        self.bn1=nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "        # Conv Layer 2\n",
    "        self.conv2=nn.Conv2d(\n",
    "            in_channels=out_ch,out_channels=out_ch,kernel_size=3,padding=1,stride=1,bias=False\n",
    "        )\n",
    "        self.bn2=nn.BatchNorm2d(out_ch)\n",
    "        \n",
    "        self.short_cut=nn.Sequential()\n",
    "        if stride!=1 or in_ch!=out_ch:\n",
    "            self.short_cut=nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_ch,out_channels=out_ch,kernel_size=1,stride=stride,bias= False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "        out=self.bn2(self.conv2(out))\n",
    "        out+=self.short_cut(x)\n",
    "        out=nn.ReLU()(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create main network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResnet(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(MyResnet, self).__init__()\n",
    "        \n",
    "        # first convolution\n",
    "        self.conv1=nn.Conv2d(\n",
    "            in_channels=3,out_channels=64,stride=1,padding=1,bias=False, kernel_size= 3\n",
    "        )\n",
    "        self.bn1=nn.BatchNorm2d(64) #out channels from previous layer\n",
    "        print('first conv done')\n",
    "        \n",
    "        self.block1=self._create_res_blocks(64,64,stride=1)\n",
    "        self.block2=self._create_res_blocks(64,128,stride=2)\n",
    "        self.block3=self._create_res_blocks(128,256,stride=2)\n",
    "        self.block4=self._create_res_blocks(256,512,stride=2)\n",
    "        self.linear=nn.Linear(512,num_classes)\n",
    "        \n",
    "    def _create_res_blocks(self,in_ch,out_ch,stride):\n",
    "            return nn.Sequential(\n",
    "                MyResblock(in_ch,out_ch,stride),\n",
    "                MyResblock(out_ch,out_ch,1)\n",
    "            )\n",
    "        \n",
    "    def forward(self,x):\n",
    "            out=nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "            out=self.block1(out)\n",
    "            out=self.block2(out)\n",
    "            out=self.block3(out)\n",
    "            out=self.block4(out)\n",
    "            out=nn.AvgPool2d(4)(out)\n",
    "            out=out.view(out.size(0), -1)\n",
    "            out=self.linear(out)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data Cifar-10\n",
    "data_dir='C:/Users/Gaurav/data/cifar/cifar/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir+'labels.txt') as label_file:\n",
    "    labels=label_file.read().split()\n",
    "    label_mapping=dict(zip(labels,list(range(len(labels)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airplane': 0,\n",
       " 'automobile': 1,\n",
       " 'bird': 2,\n",
       " 'cat': 3,\n",
       " 'deer': 4,\n",
       " 'dog': 5,\n",
       " 'frog': 6,\n",
       " 'horse': 7,\n",
       " 'ship': 8,\n",
       " 'truck': 9}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the images\n",
    "def preprocess(image):\n",
    "    image = np.array(image)\n",
    "    \n",
    "    if random.random() > 0.5:\n",
    "        image = image[::-1,:,:]\n",
    "    \n",
    "    cifar_mean = np.array([0.4914, 0.4822, 0.4465]).reshape(1,1,-1)\n",
    "    cifar_std  = np.array([0.2023, 0.1994, 0.2010]).reshape(1,1,-1)\n",
    "    image = (image - cifar_mean) / cifar_std\n",
    "    \n",
    "    image = image.transpose(2,1,0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset subclass to read the images\n",
    "# Dataset class is provided by Pytorch to index into our data\n",
    "\n",
    "class MyCustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,data_dir,data_size=0,transforms=None):\n",
    "        files=os.listdir(data_dir)\n",
    "        files=[os.path.join(data_dir,x) for x in files]\n",
    "        \n",
    "        if data_size<0 or data_size>len(files):\n",
    "            assert ('Data size should be between 0 to max files in the directory')\n",
    "        \n",
    "        if data_size==0:\n",
    "            data_size=len(files)\n",
    "        \n",
    "        self.data_size=data_size\n",
    "        self.files= random.sample(files,self.data_size)\n",
    "        self.transforms=transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image_addr=self.files[idx]\n",
    "        image=Image.open(image_addr)\n",
    "        image=preprocess(image)\n",
    "        label_name=image_addr[:-4].split(\"_\")[-1]\n",
    "        label=label_mapping[label_name]\n",
    "        \n",
    "        image=image.astype(np.float32)\n",
    "        \n",
    "        if self.transforms:\n",
    "            image=self.transforms(image)\n",
    "        \n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloader and the Dataset\n",
    "\n",
    "trainset = MyCustomDataset(data_dir+'train')\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size= 64 , shuffle= True , num_workers= 0)\n",
    "\n",
    "testset = MyCustomDataset(data_dir+'test')\n",
    "testloader = torch.utils.data.DataLoader(trainset, batch_size= 64 , shuffle= True , num_workers= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select GPU or CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first conv done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyResnet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block1): Sequential(\n",
       "    (0): MyResblock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (short_cut): Sequential()\n",
       "    )\n",
       "    (1): MyResblock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (short_cut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): MyResblock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (short_cut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): MyResblock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (short_cut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): MyResblock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (short_cut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): MyResblock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (short_cut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): MyResblock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (short_cut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): MyResblock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (short_cut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MyResnet()\n",
    "clf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(clf.parameters(),lr=0.1, momentum=0.9,weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones = [150,200], gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Index: 0 Loss : 2.337 Time : 6.059 seconds\n",
      "Batch Index: 100 Loss : 2.318 Time : 620.555 seconds\n",
      "Batch Index: 200 Loss : 2.315 Time : 1239.500 seconds\n",
      "Batch Index: 300 Loss : 2.317 Time : 5511.076 seconds\n",
      "Batch Index: 400 Loss : 2.317 Time : 9952.934 seconds\n",
      "Batch Index: 500 Loss : 2.318 Time : 11643.863 seconds\n"
     ]
    }
   ],
   "source": [
    "# Creating the training loop\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    \n",
    "    #Train\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs,targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad() # this is required so that our gradients don't accumulate after each epoch\n",
    "        \n",
    "        outputs = clf(inputs) # run the model on inputs\n",
    "        loss = criterion(outputs,targets) # calc the Loss\n",
    "        loss.backward() # Calc the gradients\n",
    "        optimizer.step() # update the parameters \n",
    "        losses.append(loss.item())\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if batch_idx % 100 ==0:\n",
    "            print('Batch Index: %d Loss : %.3f Time : %.3f seconds' %(batch_idx,np.mean(losses),end_time-start_time))\n",
    "            \n",
    "    # Eval Mode\n",
    "    clf.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = clf(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "        print('Epoch : %d Test Acc : %.3f' % (epoch, 100.*correct/total))\n",
    "        print('--------------------------------------------------------------')\n",
    "    clf.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
